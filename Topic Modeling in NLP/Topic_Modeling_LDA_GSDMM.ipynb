{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling On Translink Tweets\n",
    "\n",
    "Topic Modeling aims to find the topics (or clusters) inside a corpus of texts (like mails or news articles), without knowing those topics at first. Here lies the real power of Topic Modeling, you don’t need any labeled or annotated data, only raw texts, and from this chaos Topic Modeling algorithms will find the topics your texts are about!\n",
    "\n",
    "The most popular approach for Topic Modeling is Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does LDA do?\n",
    "\n",
    "LDA’s approach to topic modeling considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We will need to remove frequency used words that are not helpful like 'the', 'a', ... These are called  `stopwords'.\n",
    "\n",
    "We also need to lemmalize the words. Lemmatization is nothing but converting a word to its root word. For example: the lemma of the word ‘machines’ is ‘machine’. Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gsdmm.mgp import MovieGroupProcess\n",
    "from gensim.models import CoherenceModel\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['https', '@', 's', 'http', 'co', 'woman', 'man','translink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>TweetDateTime</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Friends</th>\n",
       "      <th>Statuses</th>\n",
       "      <th>Favourites</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25559</th>\n",
       "      <td>@TransLink kudos to the driver of 100 bus this...</td>\n",
       "      <td>2015-11-15 21:22</td>\n",
       "      <td>62</td>\n",
       "      <td>142</td>\n",
       "      <td>3130</td>\n",
       "      <td>9385</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251957</th>\n",
       "      <td>@scronide Thank you, will let control know.^jd</td>\n",
       "      <td>2013-11-14 2:43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84769</th>\n",
       "      <td>@TransLink first trains leaving new west stn</td>\n",
       "      <td>2016-04-10 14:57</td>\n",
       "      <td>89</td>\n",
       "      <td>610</td>\n",
       "      <td>160</td>\n",
       "      <td>214</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40636</th>\n",
       "      <td>@TransLink whats going on with 16 southbound o...</td>\n",
       "      <td>2016-12-19 3:49</td>\n",
       "      <td>225</td>\n",
       "      <td>320</td>\n",
       "      <td>209</td>\n",
       "      <td>4</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299766</th>\n",
       "      <td>@TransLink Lougheed station is so crowded righ...</td>\n",
       "      <td>2018-11-13 15:51</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TweetText     TweetDateTime  \\\n",
       "25559   @TransLink kudos to the driver of 100 bus this...  2015-11-15 21:22   \n",
       "251957     @scronide Thank you, will let control know.^jd   2013-11-14 2:43   \n",
       "84769        @TransLink first trains leaving new west stn  2016-04-10 14:57   \n",
       "40636   @TransLink whats going on with 16 southbound o...   2016-12-19 3:49   \n",
       "299766  @TransLink Lougheed station is so crowded righ...  2018-11-13 15:51   \n",
       "\n",
       "       Followers Friends Statuses Favourites Sentiment  \n",
       "25559         62     142     3130       9385  Negative  \n",
       "251957         0       0        0          0       NaN  \n",
       "84769         89     610      160        214  Positive  \n",
       "40636        225     320      209          4  Negative  \n",
       "299766        11       7        5          1   Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('translink.csv').sample(frac=1, random_state=0)\n",
    "df[['TweetText','TweetDateTime','Followers','Friends','Statuses','Favourites','Sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TransLink kudos to the driver of 100 bus this AM who refused to leave woman in distress along Marine Dr #thingsmoreimportantthanbeinglate\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.TweetText.values.tolist()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize each sentence\n",
    "\n",
    "Here we tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s simple_preprocess() is great for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['translink', 'kudos', 'the', 'driver', 'bus', 'this', 'who', 'refused', 'leave', 'woman', 'distress', 'along', 'marine']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True, min_len=3, max_len=15))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['translink', 'kudos', 'the', 'driver', 'bus', 'this', 'who', 'refused', 'leave', 'woman', 'distress', 'along', 'marine']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=200)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['driver', 'bus', 'refuse', 'leave', 'distress']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for LDA\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('bus', 1), ('distress', 1), ('driver', 1), ('leave', 1), ('refuse', 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.107*\"due\" + 0.095*\"today\" + 0.092*\"would\" + 0.073*\"need\" + 0.048*\"trip\" + '\n",
      "  '0.039*\"detail\" + 0.038*\"give\" + 0.038*\"well\" + 0.034*\"waterfront\" + '\n",
      "  '0.032*\"detour\"'),\n",
      " (1,\n",
      "  '0.195*\"train\" + 0.094*\"line\" + 0.057*\"back\" + 0.040*\"much\" + '\n",
      "  '0.033*\"feedback\" + 0.031*\"expo\" + 0.029*\"always\" + 0.020*\"nice\" + '\n",
      "  '0.019*\"area\" + 0.017*\"ride\"'),\n",
      " (2,\n",
      "  '0.201*\"thank\" + 0.119*\"time\" + 0.071*\"station\" + 0.051*\"take\" + '\n",
      "  '0.046*\"schedule\" + 0.036*\"make\" + 0.034*\"number\" + 0.026*\"may\" + '\n",
      "  '0.025*\"info\" + 0.022*\"sure\"'),\n",
      " (3,\n",
      "  '0.104*\"happen\" + 0.102*\"report\" + 0.099*\"issue\" + 0.081*\"morning\" + '\n",
      "  '0.034*\"open\" + 0.033*\"customer\" + 0.031*\"main\" + 0.030*\"yet\" + '\n",
      "  '0.025*\"operate\" + 0.023*\"put\"'),\n",
      " (4,\n",
      "  '0.077*\"route\" + 0.073*\"people\" + 0.055*\"right\" + 0.046*\"arrive\" + '\n",
      "  '0.045*\"amp\" + 0.044*\"able\" + 0.044*\"try\" + 0.041*\"have\" + 0.038*\"head\" + '\n",
      "  '0.037*\"downtown\"'),\n",
      " (5,\n",
      "  '0.099*\"get\" + 0.074*\"skytrain\" + 0.067*\"know\" + 0.065*\"s\" + 0.055*\"minute\" '\n",
      "  '+ 0.047*\"pass\" + 0.036*\"let\" + 0.032*\"control\" + 0.027*\"could\" + '\n",
      "  '0.022*\"find\"'),\n",
      " (6,\n",
      "  '0.078*\"great\" + 0.078*\"long\" + 0.066*\"really\" + 0.063*\"hope\" + '\n",
      "  '0.045*\"close\" + 0.044*\"change\" + 0.035*\"compass\" + 0.033*\"even\" + '\n",
      "  '0.029*\"hour\" + 0.026*\"information\"'),\n",
      " (7,\n",
      "  '0.112*\"late\" + 0.085*\"good\" + 0.076*\"work\" + 0.073*\"min\" + 0.066*\"hear\" + '\n",
      "  '0.064*\"day\" + 0.037*\"last\" + 0.034*\"unfortunately\" + 0.033*\"be\" + '\n",
      "  '0.030*\"currently\"'),\n",
      " (8,\n",
      "  '0.165*\"come\" + 0.154*\"still\" + 0.085*\"use\" + 0.055*\"experience\" + '\n",
      "  '0.052*\"zone\" + 0.050*\"think\" + 0.047*\"tonight\" + 0.031*\"will\" + '\n",
      "  '0.022*\"actually\" + 0.021*\"travel\"'),\n",
      " (9,\n",
      "  '0.226*\"bus\" + 0.097*\"stop\" + 0.048*\"go\" + 0.045*\"driver\" + 0.045*\"leave\" + '\n",
      "  '0.038*\"service\" + 0.027*\"next\" + 0.023*\"way\" + 0.021*\"check\" + 0.018*\"say\"'),\n",
      " (10,\n",
      "  '0.115*\"look\" + 0.101*\"jkd\" + 0.072*\"call\" + 0.069*\"car\" + 0.069*\"early\" + '\n",
      "  '0.040*\"keep\" + 0.037*\"passenger\" + 0.036*\"can\" + 0.032*\"follow\" + '\n",
      "  '0.028*\"away\"'),\n",
      " (11,\n",
      "  '0.132*\"wait\" + 0.101*\"delay\" + 0.101*\"run\" + 0.074*\"sorry\" + 0.073*\"see\" + '\n",
      "  '0.065*\"show\" + 0.035*\"full\" + 0.029*\"traffic\" + 0.027*\"send\" + '\n",
      "  '0.025*\"suppose\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.20589083447406642\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Text Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the great performance of LDA on medium or large sized texts (>50 words, typically mails and news articles are about this size range) it poorly performs on short texts like Tweets, Reddit posts or StackOverflow titles’ questions.\n",
    "\n",
    "The assumption of LDA is that a text is a mixture of topics. This is not true in the case of short texts. We will now assume that a short text is made from only one topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling Dirichlet Mixture Model (GSDMM)\n",
    "\n",
    "The Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an “altered” LDA algorithm, showing great results on STTM tasks, that makes the initial assumption: 1 topic ↔️1 document. The words within a document are generated using the same unique topic, and not from a mixture of topics as it was in the original LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 338964 clusters with 10 clusters populated\n",
      "In stage 1: transferred 315467 clusters with 10 clusters populated\n",
      "In stage 2: transferred 264938 clusters with 10 clusters populated\n",
      "In stage 3: transferred 187426 clusters with 10 clusters populated\n",
      "In stage 4: transferred 151606 clusters with 10 clusters populated\n",
      "In stage 5: transferred 139882 clusters with 10 clusters populated\n",
      "In stage 6: transferred 135618 clusters with 10 clusters populated\n",
      "In stage 7: transferred 133751 clusters with 10 clusters populated\n",
      "In stage 8: transferred 133439 clusters with 10 clusters populated\n",
      "In stage 9: transferred 132368 clusters with 10 clusters populated\n",
      "In stage 10: transferred 132399 clusters with 10 clusters populated\n",
      "In stage 11: transferred 131354 clusters with 10 clusters populated\n",
      "In stage 12: transferred 131124 clusters with 10 clusters populated\n",
      "In stage 13: transferred 130877 clusters with 10 clusters populated\n",
      "In stage 14: transferred 131035 clusters with 10 clusters populated\n",
      "In stage 15: transferred 130776 clusters with 10 clusters populated\n",
      "In stage 16: transferred 130633 clusters with 10 clusters populated\n",
      "In stage 17: transferred 130654 clusters with 10 clusters populated\n",
      "In stage 18: transferred 130511 clusters with 10 clusters populated\n",
      "In stage 19: transferred 130795 clusters with 10 clusters populated\n",
      "In stage 20: transferred 130426 clusters with 10 clusters populated\n",
      "In stage 21: transferred 130534 clusters with 10 clusters populated\n",
      "In stage 22: transferred 130415 clusters with 10 clusters populated\n",
      "In stage 23: transferred 130121 clusters with 10 clusters populated\n",
      "In stage 24: transferred 129921 clusters with 10 clusters populated\n",
      "In stage 25: transferred 129610 clusters with 10 clusters populated\n",
      "In stage 26: transferred 129654 clusters with 10 clusters populated\n",
      "In stage 27: transferred 129897 clusters with 10 clusters populated\n",
      "In stage 28: transferred 129327 clusters with 10 clusters populated\n",
      "In stage 29: transferred 129065 clusters with 10 clusters populated\n"
     ]
    }
   ],
   "source": [
    "K=10 # Number of topics\n",
    "docs=data_lemmatized\n",
    "# Train a new model \n",
    "\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "mgp = MovieGroupProcess(K=K, alpha=0.1, beta=0.1, n_iters=30)\n",
    "\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "n_docs = len(docs)\n",
    "\n",
    "# Fit the model on the data given the chosen seeds\n",
    "y = mgp.fit(docs, n_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Number 0 Most Popular Words are:\t\n",
      "['bus' 'wait' 'run' 'delay' 'due' 'sorry' 'late' 'cancel' 'stop' 'trip'\n",
      " 'report' 'time' 'min' 'leave' 'see' 'show' 'today' 'next' 'check'\n",
      " 'problem' 'long' 'issue' 'apology' 'minute' 'jkd' 'schedule'\n",
      " 'unfortunately' 'currently' 'able' 'get' 'way' 'traffic' 'arrive'\n",
      " 'really' 'running' 'morning' 'mechanical' 'make' 'still' 'go' 'hope'\n",
      " 'look' 'early' 'miss' 'amp' 'route' 'driver' 'coach' 'hear' 'couple'\n",
      " 'gps' 'approx' 'right' 'come' 'thank' 'shortly' 'hopefully' 'moment'\n",
      " 'departure' 'momentarily' 'behind_schedule' 'station' 'pass' 'close'\n",
      " 'cause' 'service' 'be' 'appear' 'depart' 'behind' 'good' 'experience'\n",
      " 'may' 'away' 'board' 'soon' 'cancellation' 'sure' 'already' 'far' 'yet'\n",
      " 'control' 'last' 'happen' 'train' 'number' 'specific' 'tonight' 'back'\n",
      " 'almost' 'response' 'result' 'earlier' 'full' 's' 'longer' 'bridgeport'\n",
      " 'downtown' 'catch' 'one']\n",
      "\n",
      "Cluster Number 1 Most Popular Words are:\t\n",
      "['bus' 'driver' 'train' 'skytrain' 'stop' 'get' 'car' 'people' 'station'\n",
      " 'door' 'thank' 'need' 'see' 's' 'passenger' 'turn' 'open' 'would' 'seat'\n",
      " 'make' 'back' 'go' 'leave' 'know' 'take' 'think' 'guy' 'right' 'heat'\n",
      " 'allow' 'could' 'ask' 'use' 'line' 'tell' 'drive' 'may' 'front' 'transit'\n",
      " 'amp' 'way' 'time' 'really' 'let' 'number' 'bike' 'look' 'say' 'come'\n",
      " 'head' 'today' 'close' 'help' 'sure' 'can' 'good' 'ride' 'give' 'want'\n",
      " 'find' 'sign' 'window' 'new' 'side' 'move' 'stand' 'sit' 'still'\n",
      " 'morning' 'try' 'feel' 'even' 'platform' 'put' 'keep' 'break' 'full'\n",
      " 'seem' 'clean' 'wait' 'also' 'smoke' 'run' 'old' 'route' 'light' 'happen'\n",
      " 'smell' 'walk' 'person' 'work' 'be' 'board' 'fall' 'bad' 'lose' 'day'\n",
      " 'hot' 'area' 'bag']\n",
      "\n",
      "Cluster Number 2 Most Popular Words are:\t\n",
      "['train' 'station' 'line' 'skytrain' 'go' 'waterfront' 'expo' 'run'\n",
      " 'leave' 's' 'last' 'take' 'get' 'delay' 'time' 'wait' 'thank' 'stop'\n",
      " 'bus' 'way' 'head' 'car' 'still' 'work' 'service' 'morning' 'tonight'\n",
      " 'millennium' 'long' 'move' 'main' 'come' 'issue' 'production' 'minute'\n",
      " 'platform' 'need' 'downtown' 'announcement' 'say' 'problem' 'open'\n",
      " 'today' 'commercial' 'track' 'door' 'happen' 'edmond' 'first' 'would'\n",
      " 'right' 'see' 'know' 'people' 'min' 'transfer' 'start' 'good' 'close'\n",
      " 'lougheed' 'back' 'metrotown' 'stadium' 'westbound' 'make' 'hear' 'late'\n",
      " 'sky' 'due' 'street' 'hold' 'report' 'side' 'bridgeport' 'tomorrow'\n",
      " 'arrive' 'shuttle' 'currently' 'escalator' 'operate' 'next' 'could'\n",
      " 'seem' 'stick' 'fix' 'burrard' 'tell' 'sign' 'gateway' 'catch'\n",
      " 'brighouse' 'elevator' 'night' 'amp' 'bind' 'turn' 'new' 'normal'\n",
      " 'loughee' 'think']\n",
      "\n",
      "Cluster Number 3 Most Popular Words are:\t\n",
      "['bus' 'stop' 'time' 'wait' 'next' 'come' 'leave' 'go' 'show' 'schedule'\n",
      " 'number' 'thank' 'minute' 'min' 'run' 'check' 'see' 's' 'happen' 'late'\n",
      " 'still' 'suppose' 'station' 'say' 'arrive' 'get' 'look' 'know' 'last'\n",
      " 'today' 'delay' 'cancel' 'route' 'right' 'waiting' 'close' 'have' 'tell'\n",
      " 'sign' 'amp' 'head' 'downtown' 'ago' 'would' 'early' 'text' 'update'\n",
      " 'take' 'yet' 'pass' 'sorry' 'where' 'service' 'detour' 'morning' 'catch'\n",
      " 'due' 'change' 'gps' 'info' 'way' 'work' 'direction' 'already' 'report'\n",
      " 'could' 'idea' 'departure' 'use' 'wonder' 'give' 'depart' 'real' 'main'\n",
      " 'currently' 'good' 'website' 'miss' 'tweet' 'long' 'mean' 'able' 'try'\n",
      " 'be' 'also' 'find' 'estimate' 'sure' 'may' 'jkd' 'metrotown' 'soon'\n",
      " 'issue' 'bridgeport' 'locate' 'driver' 'seem' 'street' 'almost' 'start']\n",
      "\n",
      "Cluster Number 4 Most Popular Words are:\t\n",
      "['thank' 'know' 'control' 'let' 'skytrain' 'info' 'call' 'report' 'look'\n",
      " 'jkd' 'number' 'contact' 'pass' 'send' 'issue' 'bus' 'check' 'happen'\n",
      " 'alert' 'sorry' 'train' 'driver' 'car' 'head' 'able' 'take' 'see' 'help'\n",
      " 'customer_relation' 'stop' 'get' 'time' 'detail' 'hear' 'information'\n",
      " 'notify' 'advise' 'aware' 'need' 'tweet' 'would' 'transit' 'compass'\n",
      " 'service' 'sure' 'also' 'could' 'customer' 'have' 'right' 'forward' 'ill'\n",
      " 'good' 'follow' 'much' 'may' 'find' 'station' 'problem' 'give'\n",
      " 'experience' 'update' 'make' 'text' 'amp' 'unfortunately' 'apology'\n",
      " 'still' 'attendant' 'really' 'way' 'provide' 'soon' 'phone' 'work' 'sign'\n",
      " 'try' 'hope' 'be' 'wait' 'away' 'want' 'inform' 'custrelation' 'digit'\n",
      " 'fix' 'receive' 'assistance' 'file' 'clean' 'additional' 'delay' 'regard'\n",
      " 'hopefully' 'coach' 'back' 'well' 'please' 'speak' 'transit_police']\n",
      "\n",
      "Cluster Number 5 Most Popular Words are:\t\n",
      "['thank' 'bus' 'good' 'get' 'time' 'day' 'work' 'great' 'help' 'make'\n",
      " 'would' 'know' 'driver' 'hope' 'service' 'see' 'much' 'transit' 'take'\n",
      " 'today' 'route' 's' 'tweet' 'try' 'sure' 'info' 'way' 'update' 'go'\n",
      " 'well' 'hear' 'find' 'be' 'night' 'need' 'morning' 'look' 'guy' 'send'\n",
      " 'think' 'change' 'say' 'have' 'could' 'may' 'people' 'check' 'happen'\n",
      " 'really' 'use' 'back' 'new' 'alert' 'want' 'give' 'appreciate' 'come'\n",
      " 'link' 'keep' 'website' 'always' 'call' 'thing' 'still' 'skytrain' 'nice'\n",
      " 'late' 'sorry' 'run' 'schedule' 'customer' 'next' 'do' 'reply' 'home'\n",
      " 'stop' 'able' 'hopefully' 'amp' 'last' 'awesome' 'glad' 'also' 'trip'\n",
      " 'system' 'problem' 'feedback' 'wait' 'happy' 'start' 'right' 'weekend'\n",
      " 'delay' 'plan' 'train' 'issue' 'response' 'information' 'tonight'\n",
      " 'question']\n",
      "\n",
      "Cluster Number 6 Most Popular Words are:\t\n",
      "['bus' 'zone' 'use' 'compass' 'pass' 'ticket' 'card' 'tap' 'get' 'pay'\n",
      " 'fare' 'would' 'still' 'skytrain' 'station' 'buy' 'machine' 'work' 'need'\n",
      " 'time' 'take' 'day' 'purchase' 'thank' 'charge' 'today' 'transfer'\n",
      " 'month' 'load' 'go' 'able' 'compass_card' 'travel' 'trip' 'try' 'transit'\n",
      " 'new' 'service' 'system' 'say' 'available' 'customer' 'call' 'amp' 'can'\n",
      " 'know' 'good' 'start' 'cash' 'give' 'find' 'gate' 'also' 'make' 'change'\n",
      " 'online' 'people' 'free' 'may' 'faresaver' 'see' 'open' 'want' 'way'\n",
      " 'could' 'issue' 'check' 'adult' 'train' 's' 'sell' 'will' 'add' 'first'\n",
      " 'phone' 'ride' 'help' 'concession' 'come' 'monthly' 'right' 'valid' 'be'\n",
      " 'option' 'close' 'back' 'cost' 'happen' 'money' 'next' 'long' 'much'\n",
      " 'last' 'expire' 'even' 'info' 'show' 'jkd' 'student' 'unfortunately']\n",
      "\n",
      "Cluster Number 7 Most Popular Words are:\t\n",
      "['bus' 'stop' 'wait' 'driver' 'late' 'go' 'get' 'time' 'minute' 'people'\n",
      " 'thank' 'leave' 'full' 'come' 'run' 's' 'min' 'take' 'show' 'route'\n",
      " 'service' 'pass' 'work' 'early' 'line' 'make' 'know' 'drive' 'need'\n",
      " 'happen' 'day' 'would' 'right' 'still' 'see' 'have' 'schedule' 'morning'\n",
      " 'today' 'always' 'station' 'next' 'even' 'say' 'hour' 'miss' 'long'\n",
      " 'passenger' 'way' 'really' 'arrive' 'tell' 'downtown' 'pick' 'let' 'last'\n",
      " 'good' 'suppose' 'back' 'walk' 'already' 'train' 'never' 'amp' 'stand'\n",
      " 'almost' 'week' 'could' 'want' 'think' 'lot' 'guy' 'cancel' 'catch'\n",
      " 'delay' 'home' 'try' 'first' 'least' 'nice' 'sign' 'ever' 'sit' 'past'\n",
      " 'many' 'start' 'look' 'give' 'empty' 'seem' 'bad' 'reason' 'change'\n",
      " 'extra' 'skytrain' 'away' 'rain' 'can' 'pull' 'second']\n",
      "\n",
      "Cluster Number 8 Most Popular Words are:\t\n",
      "['delay' 'bus' 'service' 'due' 'route' 'run' 'train' 'time' 'traffic'\n",
      " 'sorry' 'issue' 'detour' 'cause' 'report' 'still' 'wait' 'regular'\n",
      " 'schedule' 'today' 'stop' 'back' 'unfortunately' 'skytrain' 'problem'\n",
      " 'line' 'operate' 'thank' 'go' 'jkd' 'may' 'morning' 'clear' 'experience'\n",
      " 'see' 'get' 'long' 'construction' 'accident' 'currently' 'downtown'\n",
      " 'early' 'area' 'apology' 'work' 'really' 'tomorrow' 'normal' 'amp'\n",
      " 'affect' 'info' 'close' 'take' 'check' 'change' 'expect' 'track'\n",
      " 'station' 'move' 'start' 'min' 'good' 'tonight' 'try' 'right' 'happen'\n",
      " 'major' 's' 'earlier' 'road' 'update' 'bridge' 'main' 'would' 'yet' 'way'\n",
      " 'block' 'news' 'full' 'hear' 'minute' 'extra' 'day' 'late' 'system'\n",
      " 'look' 'gap' 'maintenance' 'moment' 'need' 'hour' 'direction' 'use'\n",
      " 'also' 'head' 'could' 'plan' 'come' 'detail' 'weather' 'show']\n",
      "\n",
      "Cluster Number 9 Most Popular Words are:\t\n",
      "['thank' 'great' 'hear' 'good' 'feedback' 'leave' 'detail' 'day' 'hope'\n",
      " 'experience' 'time' 'customer_relation' 'would' 'happen' 'sorry' 'driver'\n",
      " 'service' 'jkd' 'welcome' 'way' 'file' 'suggestion' 'pass' 'report'\n",
      " 'evening' 'night' 'take' 'commendation' 'fill' 'really' 'link' 's' 'know'\n",
      " 'bus' 'share' 'rest' 'request' 'submit' 'like' 'sure' 'appreciate' 'send'\n",
      " 'enjoy' 'let' 'today' 'afternoon' 'glad' 'look' 'well' 'provide'\n",
      " 'take_moment' 'custrelation' 'make' 'contact' 'wait' 'customer'\n",
      " 'increase' 'much' 'complaint' 'wish' 'regard' 'follow' 'nice' 'give'\n",
      " 'forward' 'be' 'feedback_form' 'wonderful' 'apology' 'info' 'see' 'delay'\n",
      " 'better' 'people' 'weekend' 'issue' 'online' 'call' 'help' 'long' 'form'\n",
      " 'planning_scheduling' 'department' 'also' 'want' 'moment' 'able' 'get'\n",
      " 'frustration' 'full' 'tweet' 'speak' 'may' 'concern' 'problem' 'morning'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'jrlezfsrpu' 'change' 'dept' 'could']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Cluster Number'+' '+str(i)+' '+'Most Popular Words are:'+'\\t')\n",
    "    arg=np.argsort(-np.array([v for v in mgp.cluster_word_distribution[i].values()]))[0:100]\n",
    "    print(np.array([k for k in mgp.cluster_word_distribution[i].keys()])[arg])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topics : [32377 32307 39365 48616 31391 53049 29411 53943 35514 28915]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [7 5 3 2 8 0 1 4 6 9]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be hand made so the topic names match the above clusters regarding their content\n",
    "topic_dict = {}\n",
    "topic_names = ['bus delay','bus issues like bad smell or door not working', 'delay in skytrain stations', 'bus rout change', 'police control at skytrain ','appreciation, good service', 'transit fare', 'bus is full','bus delay due to an issue like construction or weather' ,'translink appreciates feedback']\n",
    "\n",
    "for i, topic_num in enumerate(top_index):\n",
    "    topic_dict[topic_num]=topic_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus delay: Most Popular Words are:\t\n",
      "['bus' 'wait' 'run' 'delay' 'due' 'sorry' 'late' 'cancel' 'stop' 'trip']\n",
      "\n",
      "bus issues like bad smell or door not working: Most Popular Words are:\t\n",
      "['bus' 'driver' 'train' 'skytrain' 'stop' 'get' 'car' 'people' 'station'\n",
      " 'door']\n",
      "\n",
      "delay in skytrain stations: Most Popular Words are:\t\n",
      "['train' 'station' 'line' 'skytrain' 'go' 'waterfront' 'expo' 'run'\n",
      " 'leave' 's']\n",
      "\n",
      "bus rout change: Most Popular Words are:\t\n",
      "['bus' 'stop' 'time' 'wait' 'next' 'come' 'leave' 'go' 'show' 'schedule']\n",
      "\n",
      "police control at skytrain : Most Popular Words are:\t\n",
      "['thank' 'know' 'control' 'let' 'skytrain' 'info' 'call' 'report' 'look'\n",
      " 'jkd']\n",
      "\n",
      "appreciation, good service: Most Popular Words are:\t\n",
      "['thank' 'bus' 'good' 'get' 'time' 'day' 'work' 'great' 'help' 'make']\n",
      "\n",
      "transit fare: Most Popular Words are:\t\n",
      "['bus' 'zone' 'use' 'compass' 'pass' 'ticket' 'card' 'tap' 'get' 'pay']\n",
      "\n",
      "bus is full: Most Popular Words are:\t\n",
      "['bus' 'stop' 'wait' 'driver' 'late' 'go' 'get' 'time' 'minute' 'people']\n",
      "\n",
      "bus delay due to an issue like construction or weather: Most Popular Words are:\t\n",
      "['delay' 'bus' 'service' 'due' 'route' 'run' 'train' 'time' 'traffic'\n",
      " 'sorry']\n",
      "\n",
      "translink appreciates feedback: Most Popular Words are:\t\n",
      "['thank' 'great' 'hear' 'good' 'feedback' 'leave' 'detail' 'day' 'hope'\n",
      " 'experience']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(topic_names[i]+': '+'Most Popular Words are:'+'\\t')\n",
    "    arg=np.argsort(-np.array([v for v in mgp.cluster_word_distribution[i].values()]))[0:10]\n",
    "    print(np.array([k for k in mgp.cluster_word_distribution[i].keys()])[arg])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refrences\n",
    "https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "https://github.com/rwalk/gsdmm\n",
    "https://github.com/mamrou/short_text_topic_modeling/blob/master/notebook_sttm_example.ipynb\n",
    "https://www.coursera.org/learn/python-text-mining\n",
    "https://www.coursera.org/learn/language-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
