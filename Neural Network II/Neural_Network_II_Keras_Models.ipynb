{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, add, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv('data_banknote_authentication.txt',\n",
    "                           names=['Variance of Wavelet Transformed Image',\n",
    "                                  'Skewness of Wavelet Transformed Image',\n",
    "                                  'Curtosis of Wavelet Transformed Image',\n",
    "                                  'Entropy of Image',\n",
    "                                  'Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define necessary functions\n",
    "Here I define f1 score to be used as metric due to slight class imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(d):\n",
    "    # d is a (n x dimension) np array\n",
    "    d -= np.min(d, axis=0)\n",
    "    d /= np.ptp(d, axis=0)\n",
    "    return (d*0.6)+0.2\n",
    "\n",
    "# Or use MinMaxScaler:\n",
    "scaler = MinMaxScaler(feature_range=(0.2, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Here, I use one hot encoding for the targets in train set. I also standerdize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input_train, input_test=train_test_split(input, test_size=0.15)\n",
    "y = df_input['Class'].values.reshape(-1,1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "#Or y = keras.utils.to_categorical(y, num_classes=2)\n",
    "\n",
    "x= df_input[['Variance of Wavelet Transformed Image',\n",
    "                                  'Skewness of Wavelet Transformed Image',\n",
    "                                  'Curtosis of Wavelet Transformed Image',\n",
    "                                  'Entropy of Image']].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.15, random_state=0)\n",
    "\n",
    "# Fit scaler only to train set to avoid leekage\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "\n",
    "y_train=onehot_encoder.fit_transform(y_train)\n",
    "dim1, dim2 = np.shape(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are slightly imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist=plt.hist(df_input['Class'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sequential model:\n",
    "I played around with different parameters and I found that the relu activation function in the hidden layers works better than the sigmoid. I also use softmax in the output. There is not much difference between softmax and sigmoid as targets are hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 932 samples, validate on 234 samples\n",
      "Epoch 1/40\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.6986 - acc: 0.5043 - f1: 0.5043 - val_loss: 0.6836 - val_acc: 0.5983 - val_f1: 0.5983\n",
      "Epoch 2/40\n",
      "932/932 [==============================] - 0s 72us/step - loss: 0.6830 - acc: 0.5397 - f1: 0.5397 - val_loss: 0.6750 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 3/40\n",
      "932/932 [==============================] - 0s 70us/step - loss: 0.6739 - acc: 0.5408 - f1: 0.5408 - val_loss: 0.6673 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 4/40\n",
      "932/932 [==============================] - 0s 78us/step - loss: 0.6614 - acc: 0.5526 - f1: 0.5526 - val_loss: 0.6571 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 5/40\n",
      "932/932 [==============================] - 0s 82us/step - loss: 0.6514 - acc: 0.5697 - f1: 0.5697 - val_loss: 0.6456 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 6/40\n",
      "932/932 [==============================] - 0s 77us/step - loss: 0.6438 - acc: 0.5858 - f1: 0.5858 - val_loss: 0.6329 - val_acc: 0.5556 - val_f1: 0.5556\n",
      "Epoch 7/40\n",
      "932/932 [==============================] - 0s 75us/step - loss: 0.6315 - acc: 0.6041 - f1: 0.6041 - val_loss: 0.6191 - val_acc: 0.6410 - val_f1: 0.6410\n",
      "Epoch 8/40\n",
      "932/932 [==============================] - 0s 82us/step - loss: 0.6217 - acc: 0.6642 - f1: 0.6642 - val_loss: 0.6072 - val_acc: 0.6068 - val_f1: 0.6068\n",
      "Epoch 9/40\n",
      "932/932 [==============================] - 0s 75us/step - loss: 0.6155 - acc: 0.6577 - f1: 0.6577 - val_loss: 0.5926 - val_acc: 0.6667 - val_f1: 0.6667\n",
      "Epoch 10/40\n",
      "932/932 [==============================] - 0s 77us/step - loss: 0.6005 - acc: 0.6996 - f1: 0.6996 - val_loss: 0.5769 - val_acc: 0.7265 - val_f1: 0.7265\n",
      "Epoch 11/40\n",
      "932/932 [==============================] - 0s 75us/step - loss: 0.5816 - acc: 0.7103 - f1: 0.7103 - val_loss: 0.5615 - val_acc: 0.7906 - val_f1: 0.7906\n",
      "Epoch 12/40\n",
      "932/932 [==============================] - 0s 71us/step - loss: 0.5729 - acc: 0.7371 - f1: 0.7371 - val_loss: 0.5433 - val_acc: 0.8034 - val_f1: 0.8034\n",
      "Epoch 13/40\n",
      "932/932 [==============================] - 0s 72us/step - loss: 0.5636 - acc: 0.7436 - f1: 0.7436 - val_loss: 0.5345 - val_acc: 0.8803 - val_f1: 0.8803\n",
      "Epoch 14/40\n",
      "932/932 [==============================] - 0s 74us/step - loss: 0.5468 - acc: 0.7704 - f1: 0.7704 - val_loss: 0.5074 - val_acc: 0.8077 - val_f1: 0.8077\n",
      "Epoch 15/40\n",
      "932/932 [==============================] - 0s 87us/step - loss: 0.5236 - acc: 0.7876 - f1: 0.7876 - val_loss: 0.4898 - val_acc: 0.8248 - val_f1: 0.8248\n",
      "Epoch 16/40\n",
      "932/932 [==============================] - 0s 87us/step - loss: 0.5095 - acc: 0.7886 - f1: 0.7886 - val_loss: 0.4780 - val_acc: 0.7991 - val_f1: 0.7991\n",
      "Epoch 17/40\n",
      "932/932 [==============================] - 0s 100us/step - loss: 0.5093 - acc: 0.7747 - f1: 0.7747 - val_loss: 0.4619 - val_acc: 0.8205 - val_f1: 0.8205\n",
      "Epoch 18/40\n",
      "932/932 [==============================] - 0s 109us/step - loss: 0.4805 - acc: 0.8079 - f1: 0.8079 - val_loss: 0.4558 - val_acc: 0.7949 - val_f1: 0.7949\n",
      "Epoch 19/40\n",
      "932/932 [==============================] - 0s 96us/step - loss: 0.4898 - acc: 0.7918 - f1: 0.7918 - val_loss: 0.4430 - val_acc: 0.7991 - val_f1: 0.7991\n",
      "Epoch 20/40\n",
      "932/932 [==============================] - 0s 86us/step - loss: 0.4712 - acc: 0.8133 - f1: 0.8133 - val_loss: 0.4286 - val_acc: 0.8248 - val_f1: 0.8248\n",
      "Epoch 21/40\n",
      "932/932 [==============================] - 0s 80us/step - loss: 0.4571 - acc: 0.8262 - f1: 0.8262 - val_loss: 0.4137 - val_acc: 0.8333 - val_f1: 0.8333\n",
      "Epoch 22/40\n",
      "932/932 [==============================] - 0s 104us/step - loss: 0.4481 - acc: 0.8444 - f1: 0.8444 - val_loss: 0.3993 - val_acc: 0.8590 - val_f1: 0.8590\n",
      "Epoch 23/40\n",
      "932/932 [==============================] - 0s 111us/step - loss: 0.4291 - acc: 0.8433 - f1: 0.8433 - val_loss: 0.3900 - val_acc: 0.8632 - val_f1: 0.8632\n",
      "Epoch 24/40\n",
      "932/932 [==============================] - 0s 87us/step - loss: 0.4139 - acc: 0.8444 - f1: 0.8444 - val_loss: 0.3855 - val_acc: 0.8419 - val_f1: 0.8419\n",
      "Epoch 25/40\n",
      "932/932 [==============================] - 0s 89us/step - loss: 0.4143 - acc: 0.8487 - f1: 0.8487 - val_loss: 0.3643 - val_acc: 0.8761 - val_f1: 0.8761\n",
      "Epoch 26/40\n",
      "932/932 [==============================] - 0s 97us/step - loss: 0.4011 - acc: 0.8594 - f1: 0.8594 - val_loss: 0.3432 - val_acc: 0.9103 - val_f1: 0.9103\n",
      "Epoch 27/40\n",
      "932/932 [==============================] - 0s 79us/step - loss: 0.3991 - acc: 0.8541 - f1: 0.8541 - val_loss: 0.3380 - val_acc: 0.9017 - val_f1: 0.9017\n",
      "Epoch 28/40\n",
      "932/932 [==============================] - 0s 93us/step - loss: 0.3821 - acc: 0.8648 - f1: 0.8648 - val_loss: 0.3466 - val_acc: 0.8761 - val_f1: 0.8761\n",
      "Epoch 29/40\n",
      "932/932 [==============================] - 0s 104us/step - loss: 0.3656 - acc: 0.8723 - f1: 0.8723 - val_loss: 0.3288 - val_acc: 0.8932 - val_f1: 0.8932\n",
      "Epoch 30/40\n",
      "932/932 [==============================] - 0s 100us/step - loss: 0.3500 - acc: 0.8777 - f1: 0.8777 - val_loss: 0.3156 - val_acc: 0.8932 - val_f1: 0.8932\n",
      "Epoch 31/40\n",
      "932/932 [==============================] - 0s 91us/step - loss: 0.3423 - acc: 0.8798 - f1: 0.8798 - val_loss: 0.2971 - val_acc: 0.9103 - val_f1: 0.9103\n",
      "Epoch 32/40\n",
      "932/932 [==============================] - 0s 88us/step - loss: 0.3489 - acc: 0.8691 - f1: 0.8691 - val_loss: 0.2879 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 33/40\n",
      "932/932 [==============================] - 0s 119us/step - loss: 0.3412 - acc: 0.8830 - f1: 0.8830 - val_loss: 0.2759 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 34/40\n",
      "932/932 [==============================] - 0s 89us/step - loss: 0.3446 - acc: 0.8734 - f1: 0.8734 - val_loss: 0.2782 - val_acc: 0.9145 - val_f1: 0.9145\n",
      "Epoch 35/40\n",
      "932/932 [==============================] - 0s 91us/step - loss: 0.3322 - acc: 0.8734 - f1: 0.8734 - val_loss: 0.2665 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 36/40\n",
      "932/932 [==============================] - 0s 82us/step - loss: 0.3233 - acc: 0.8820 - f1: 0.8820 - val_loss: 0.2794 - val_acc: 0.9017 - val_f1: 0.9017\n",
      "Epoch 37/40\n",
      "932/932 [==============================] - 0s 120us/step - loss: 0.3145 - acc: 0.8938 - f1: 0.8938 - val_loss: 0.2514 - val_acc: 0.9274 - val_f1: 0.9274\n",
      "Epoch 38/40\n",
      "932/932 [==============================] - 0s 96us/step - loss: 0.3029 - acc: 0.8916 - f1: 0.8916 - val_loss: 0.2469 - val_acc: 0.9402 - val_f1: 0.9402\n",
      "Epoch 39/40\n",
      "932/932 [==============================] - 0s 90us/step - loss: 0.2916 - acc: 0.9013 - f1: 0.9013 - val_loss: 0.2383 - val_acc: 0.9402 - val_f1: 0.9402\n",
      "Epoch 40/40\n",
      "932/932 [==============================] - 0s 140us/step - loss: 0.2865 - acc: 0.8981 - f1: 0.8981 - val_loss: 0.2388 - val_acc: 0.9359 - val_f1: 0.9359\n"
     ]
    }
   ],
   "source": [
    "#the number of neurons n in the hidden layers (which are Dense layers) is 10,\n",
    "# with sigmoid activation functions. The dropout out ratio is 0.25.\n",
    "# The final output layer is also a Dense layer with n = 1 and softmax activation.\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_dim= dim2))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#Compilation\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer='Adadelta',\n",
    "                metrics=['accuracy', f1])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(x_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "results = model.predict_classes(x_test, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113,   4],\n",
       "       [  8,  81]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models (one with and the other without residual connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 932 samples, validate on 234 samples\n",
      "Epoch 1/40\n",
      "932/932 [==============================] - 1s 1ms/step - loss: 0.6727 - acc: 0.5622 - f1: 0.5622 - val_loss: 0.6840 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 2/40\n",
      "932/932 [==============================] - 0s 117us/step - loss: 0.6650 - acc: 0.5590 - f1: 0.5590 - val_loss: 0.6651 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 3/40\n",
      "932/932 [==============================] - 0s 112us/step - loss: 0.6486 - acc: 0.5590 - f1: 0.5590 - val_loss: 0.6536 - val_acc: 0.5299 - val_f1: 0.5299\n",
      "Epoch 4/40\n",
      "932/932 [==============================] - 0s 89us/step - loss: 0.6323 - acc: 0.5601 - f1: 0.5601 - val_loss: 0.6250 - val_acc: 0.5556 - val_f1: 0.5556\n",
      "Epoch 5/40\n",
      "932/932 [==============================] - 0s 96us/step - loss: 0.6038 - acc: 0.6019 - f1: 0.6019 - val_loss: 0.6002 - val_acc: 0.5983 - val_f1: 0.5983\n",
      "Epoch 6/40\n",
      "932/932 [==============================] - 0s 111us/step - loss: 0.5707 - acc: 0.6781 - f1: 0.6781 - val_loss: 0.5620 - val_acc: 0.8205 - val_f1: 0.8205\n",
      "Epoch 7/40\n",
      "932/932 [==============================] - 0s 100us/step - loss: 0.5291 - acc: 0.8004 - f1: 0.8004 - val_loss: 0.5299 - val_acc: 0.6966 - val_f1: 0.6966\n",
      "Epoch 8/40\n",
      "932/932 [==============================] - 0s 103us/step - loss: 0.4816 - acc: 0.8423 - f1: 0.8423 - val_loss: 0.4512 - val_acc: 0.8718 - val_f1: 0.8718\n",
      "Epoch 9/40\n",
      "932/932 [==============================] - 0s 98us/step - loss: 0.4314 - acc: 0.8594 - f1: 0.8594 - val_loss: 0.4104 - val_acc: 0.8675 - val_f1: 0.8675\n",
      "Epoch 10/40\n",
      "932/932 [==============================] - 0s 102us/step - loss: 0.3942 - acc: 0.8670 - f1: 0.8670 - val_loss: 0.4012 - val_acc: 0.8205 - val_f1: 0.8205\n",
      "Epoch 11/40\n",
      "932/932 [==============================] - 0s 115us/step - loss: 0.3699 - acc: 0.8691 - f1: 0.8691 - val_loss: 0.3427 - val_acc: 0.8932 - val_f1: 0.8932\n",
      "Epoch 12/40\n",
      "932/932 [==============================] - 0s 102us/step - loss: 0.3490 - acc: 0.8766 - f1: 0.8766 - val_loss: 0.3334 - val_acc: 0.8761 - val_f1: 0.8761\n",
      "Epoch 13/40\n",
      "932/932 [==============================] - 0s 133us/step - loss: 0.3294 - acc: 0.8798 - f1: 0.8798 - val_loss: 0.2984 - val_acc: 0.8889 - val_f1: 0.8889\n",
      "Epoch 14/40\n",
      "932/932 [==============================] - 0s 111us/step - loss: 0.3034 - acc: 0.8938 - f1: 0.8938 - val_loss: 0.2750 - val_acc: 0.9103 - val_f1: 0.9103\n",
      "Epoch 15/40\n",
      "932/932 [==============================] - 0s 89us/step - loss: 0.2942 - acc: 0.8927 - f1: 0.8927 - val_loss: 0.2730 - val_acc: 0.9017 - val_f1: 0.9017\n",
      "Epoch 16/40\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.8922 - f1: 0.89 - 0s 93us/step - loss: 0.2766 - acc: 0.9077 - f1: 0.9077 - val_loss: 0.2446 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 17/40\n",
      "932/932 [==============================] - 0s 134us/step - loss: 0.2600 - acc: 0.9142 - f1: 0.9142 - val_loss: 0.2320 - val_acc: 0.9359 - val_f1: 0.9359\n",
      "Epoch 18/40\n",
      "932/932 [==============================] - 0s 128us/step - loss: 0.2537 - acc: 0.9131 - f1: 0.9131 - val_loss: 0.3021 - val_acc: 0.8846 - val_f1: 0.8846\n",
      "Epoch 19/40\n",
      "932/932 [==============================] - 0s 86us/step - loss: 0.2356 - acc: 0.9270 - f1: 0.9270 - val_loss: 0.3089 - val_acc: 0.8547 - val_f1: 0.8547\n",
      "Epoch 20/40\n",
      "932/932 [==============================] - 0s 100us/step - loss: 0.2332 - acc: 0.9174 - f1: 0.9174 - val_loss: 0.2081 - val_acc: 0.9444 - val_f1: 0.9444\n",
      "Epoch 21/40\n",
      "932/932 [==============================] - 0s 135us/step - loss: 0.2189 - acc: 0.9313 - f1: 0.9313 - val_loss: 0.1868 - val_acc: 0.9487 - val_f1: 0.9487\n",
      "Epoch 22/40\n",
      "932/932 [==============================] - 0s 107us/step - loss: 0.2105 - acc: 0.9249 - f1: 0.9249 - val_loss: 0.2455 - val_acc: 0.8974 - val_f1: 0.8974\n",
      "Epoch 23/40\n",
      "932/932 [==============================] - 0s 87us/step - loss: 0.2014 - acc: 0.9249 - f1: 0.9249 - val_loss: 0.2236 - val_acc: 0.9060 - val_f1: 0.9060\n",
      "Epoch 24/40\n",
      "932/932 [==============================] - 0s 168us/step - loss: 0.1921 - acc: 0.9356 - f1: 0.9356 - val_loss: 0.1573 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 25/40\n",
      "932/932 [==============================] - 0s 144us/step - loss: 0.1887 - acc: 0.9367 - f1: 0.9367 - val_loss: 0.1584 - val_acc: 0.9487 - val_f1: 0.9487\n",
      "Epoch 26/40\n",
      "932/932 [==============================] - 0s 104us/step - loss: 0.1803 - acc: 0.9410 - f1: 0.9410 - val_loss: 0.1451 - val_acc: 0.9444 - val_f1: 0.9444\n",
      "Epoch 27/40\n",
      "932/932 [==============================] - 0s 88us/step - loss: 0.1671 - acc: 0.9356 - f1: 0.9356 - val_loss: 0.1400 - val_acc: 0.9487 - val_f1: 0.9487\n",
      "Epoch 28/40\n",
      "932/932 [==============================] - 0s 118us/step - loss: 0.1564 - acc: 0.9464 - f1: 0.9464 - val_loss: 0.1605 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 29/40\n",
      "932/932 [==============================] - 0s 189us/step - loss: 0.1540 - acc: 0.9485 - f1: 0.9485 - val_loss: 0.1191 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 30/40\n",
      "932/932 [==============================] - 0s 87us/step - loss: 0.1466 - acc: 0.9485 - f1: 0.9485 - val_loss: 0.2162 - val_acc: 0.9060 - val_f1: 0.9060\n",
      "Epoch 31/40\n",
      "932/932 [==============================] - 0s 93us/step - loss: 0.1470 - acc: 0.9464 - f1: 0.9464 - val_loss: 0.1192 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 32/40\n",
      "932/932 [==============================] - 0s 116us/step - loss: 0.1378 - acc: 0.9549 - f1: 0.9549 - val_loss: 0.1774 - val_acc: 0.9274 - val_f1: 0.9274\n",
      "Epoch 33/40\n",
      "932/932 [==============================] - 0s 105us/step - loss: 0.1315 - acc: 0.9614 - f1: 0.9614 - val_loss: 0.2234 - val_acc: 0.9060 - val_f1: 0.9060\n",
      "Epoch 34/40\n",
      "932/932 [==============================] - 0s 107us/step - loss: 0.1293 - acc: 0.9571 - f1: 0.9571 - val_loss: 0.1235 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 35/40\n",
      "932/932 [==============================] - 0s 109us/step - loss: 0.1254 - acc: 0.9603 - f1: 0.9603 - val_loss: 0.1591 - val_acc: 0.9444 - val_f1: 0.9444\n",
      "Epoch 36/40\n",
      "932/932 [==============================] - 0s 90us/step - loss: 0.1186 - acc: 0.9657 - f1: 0.9657 - val_loss: 0.1641 - val_acc: 0.9359 - val_f1: 0.9359\n",
      "Epoch 37/40\n",
      "932/932 [==============================] - 0s 86us/step - loss: 0.1123 - acc: 0.9614 - f1: 0.9614 - val_loss: 0.0976 - val_acc: 0.9658 - val_f1: 0.9658\n",
      "Epoch 38/40\n",
      "932/932 [==============================] - 0s 74us/step - loss: 0.1067 - acc: 0.9646 - f1: 0.9646 - val_loss: 0.0823 - val_acc: 0.9744 - val_f1: 0.9744\n",
      "Epoch 39/40\n",
      "932/932 [==============================] - 0s 73us/step - loss: 0.1023 - acc: 0.9667 - f1: 0.9667 - val_loss: 0.0825 - val_acc: 0.9701 - val_f1: 0.9701\n",
      "Epoch 40/40\n",
      "932/932 [==============================] - 0s 72us/step - loss: 0.0996 - acc: 0.9700 - f1: 0.9700 - val_loss: 0.0728 - val_acc: 0.9786 - val_f1: 0.9786\n",
      "Train on 932 samples, validate on 234 samples\n",
      "Epoch 1/40\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.6652 - acc: 0.5290 - f1: 0.5290 - val_loss: 0.5965 - val_acc: 0.9231 - val_f1: 0.9231\n",
      "Epoch 2/40\n",
      "932/932 [==============================] - 0s 109us/step - loss: 0.5816 - acc: 0.7157 - f1: 0.7157 - val_loss: 0.5528 - val_acc: 0.9231 - val_f1: 0.9231\n",
      "Epoch 3/40\n",
      "932/932 [==============================] - 0s 132us/step - loss: 0.5450 - acc: 0.7618 - f1: 0.7618 - val_loss: 0.5110 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 4/40\n",
      "932/932 [==============================] - 0s 158us/step - loss: 0.5072 - acc: 0.7833 - f1: 0.7833 - val_loss: 0.4682 - val_acc: 0.9615 - val_f1: 0.9615\n",
      "Epoch 5/40\n",
      "932/932 [==============================] - 0s 105us/step - loss: 0.4662 - acc: 0.8262 - f1: 0.8262 - val_loss: 0.4253 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 6/40\n",
      "932/932 [==============================] - 0s 110us/step - loss: 0.4245 - acc: 0.8251 - f1: 0.8251 - val_loss: 0.3837 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 7/40\n",
      "932/932 [==============================] - 0s 111us/step - loss: 0.4067 - acc: 0.8369 - f1: 0.8369 - val_loss: 0.3591 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 8/40\n",
      "932/932 [==============================] - 0s 107us/step - loss: 0.3687 - acc: 0.8670 - f1: 0.8670 - val_loss: 0.3220 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 9/40\n",
      "932/932 [==============================] - 0s 96us/step - loss: 0.3643 - acc: 0.8476 - f1: 0.8476 - val_loss: 0.2915 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932/932 [==============================] - 0s 116us/step - loss: 0.3289 - acc: 0.8788 - f1: 0.8788 - val_loss: 0.2673 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 11/40\n",
      "932/932 [==============================] - 0s 72us/step - loss: 0.3281 - acc: 0.8863 - f1: 0.8863 - val_loss: 0.2408 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 12/40\n",
      "932/932 [==============================] - 0s 91us/step - loss: 0.2893 - acc: 0.9024 - f1: 0.9024 - val_loss: 0.2290 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 13/40\n",
      "932/932 [==============================] - 0s 82us/step - loss: 0.2768 - acc: 0.9185 - f1: 0.9185 - val_loss: 0.2000 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 14/40\n",
      "932/932 [==============================] - 0s 74us/step - loss: 0.2709 - acc: 0.9142 - f1: 0.9142 - val_loss: 0.1951 - val_acc: 0.9744 - val_f1: 0.9744\n",
      "Epoch 15/40\n",
      "932/932 [==============================] - 0s 75us/step - loss: 0.2541 - acc: 0.9131 - f1: 0.9131 - val_loss: 0.1939 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 16/40\n",
      "932/932 [==============================] - 0s 74us/step - loss: 0.2338 - acc: 0.9356 - f1: 0.9356 - val_loss: 0.1965 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 17/40\n",
      "932/932 [==============================] - 0s 73us/step - loss: 0.2289 - acc: 0.9292 - f1: 0.9292 - val_loss: 0.1848 - val_acc: 0.9573 - val_f1: 0.9573\n",
      "Epoch 18/40\n",
      "932/932 [==============================] - 0s 68us/step - loss: 0.2290 - acc: 0.9421 - f1: 0.9421 - val_loss: 0.1828 - val_acc: 0.9530 - val_f1: 0.9530\n",
      "Epoch 19/40\n",
      "932/932 [==============================] - 0s 67us/step - loss: 0.2200 - acc: 0.9453 - f1: 0.9453 - val_loss: 0.1306 - val_acc: 0.9786 - val_f1: 0.9786\n",
      "Epoch 20/40\n",
      "932/932 [==============================] - 0s 68us/step - loss: 0.2134 - acc: 0.9431 - f1: 0.9431 - val_loss: 0.1230 - val_acc: 0.9872 - val_f1: 0.9872\n",
      "Epoch 21/40\n",
      "932/932 [==============================] - 0s 70us/step - loss: 0.1989 - acc: 0.9453 - f1: 0.9453 - val_loss: 0.1167 - val_acc: 0.9829 - val_f1: 0.9829\n",
      "Epoch 22/40\n",
      "932/932 [==============================] - 0s 70us/step - loss: 0.1945 - acc: 0.9442 - f1: 0.9442 - val_loss: 0.1079 - val_acc: 0.9915 - val_f1: 0.9915\n",
      "Epoch 23/40\n",
      "932/932 [==============================] - 0s 68us/step - loss: 0.1847 - acc: 0.9549 - f1: 0.9549 - val_loss: 0.1029 - val_acc: 0.9829 - val_f1: 0.9829\n",
      "Epoch 24/40\n",
      "932/932 [==============================] - 0s 70us/step - loss: 0.1771 - acc: 0.9528 - f1: 0.9528 - val_loss: 0.1099 - val_acc: 0.9744 - val_f1: 0.9744\n",
      "Epoch 25/40\n",
      "932/932 [==============================] - 0s 80us/step - loss: 0.1836 - acc: 0.9496 - f1: 0.9496 - val_loss: 0.0900 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 26/40\n",
      "932/932 [==============================] - 0s 95us/step - loss: 0.1771 - acc: 0.9603 - f1: 0.9603 - val_loss: 0.0877 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 27/40\n",
      "932/932 [==============================] - 0s 93us/step - loss: 0.1681 - acc: 0.9560 - f1: 0.9560 - val_loss: 0.0836 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 28/40\n",
      "932/932 [==============================] - 0s 136us/step - loss: 0.1667 - acc: 0.9485 - f1: 0.9485 - val_loss: 0.0837 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 29/40\n",
      "932/932 [==============================] - 0s 101us/step - loss: 0.1680 - acc: 0.9549 - f1: 0.9549 - val_loss: 0.0799 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 30/40\n",
      "932/932 [==============================] - 0s 120us/step - loss: 0.1761 - acc: 0.9431 - f1: 0.9431 - val_loss: 0.1004 - val_acc: 0.9658 - val_f1: 0.9658\n",
      "Epoch 31/40\n",
      "932/932 [==============================] - 0s 238us/step - loss: 0.1462 - acc: 0.9624 - f1: 0.9624 - val_loss: 0.0742 - val_acc: 0.9915 - val_f1: 0.9915\n",
      "Epoch 32/40\n",
      "932/932 [==============================] - 0s 219us/step - loss: 0.1426 - acc: 0.9667 - f1: 0.9667 - val_loss: 0.0638 - val_acc: 0.9915 - val_f1: 0.9915\n",
      "Epoch 33/40\n",
      "932/932 [==============================] - 0s 128us/step - loss: 0.1466 - acc: 0.9624 - f1: 0.9624 - val_loss: 0.0680 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 34/40\n",
      "932/932 [==============================] - 0s 148us/step - loss: 0.1338 - acc: 0.9678 - f1: 0.9678 - val_loss: 0.1547 - val_acc: 0.9316 - val_f1: 0.9316\n",
      "Epoch 35/40\n",
      "932/932 [==============================] - 0s 154us/step - loss: 0.1420 - acc: 0.9710 - f1: 0.9710 - val_loss: 0.1315 - val_acc: 0.9487 - val_f1: 0.9487\n",
      "Epoch 36/40\n",
      "932/932 [==============================] - 0s 170us/step - loss: 0.1564 - acc: 0.9485 - f1: 0.9485 - val_loss: 0.0628 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 37/40\n",
      "932/932 [==============================] - 0s 147us/step - loss: 0.1374 - acc: 0.9528 - f1: 0.9528 - val_loss: 0.0534 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 38/40\n",
      "932/932 [==============================] - 0s 157us/step - loss: 0.1290 - acc: 0.9689 - f1: 0.9689 - val_loss: 0.0761 - val_acc: 0.9829 - val_f1: 0.9829\n",
      "Epoch 39/40\n",
      "932/932 [==============================] - 0s 137us/step - loss: 0.1343 - acc: 0.9603 - f1: 0.9603 - val_loss: 0.0492 - val_acc: 0.9957 - val_f1: 0.9957\n",
      "Epoch 40/40\n",
      "932/932 [==============================] - 0s 162us/step - loss: 0.1334 - acc: 0.9678 - f1: 0.9678 - val_loss: 0.0499 - val_acc: 1.0000 - val_f1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "inputs = Input(shape=(dim2,))\n",
    "\n",
    "l1 = Dense(10, activation='relu')(inputs)\n",
    "l2 = Dense(10, activation='relu')(l1)\n",
    "l3 = Dropout(0.25)(l2)\n",
    "x=concatenate([l2, l3])\n",
    "predictions1 = Dense(2, activation='softmax')(x) #residual connections\n",
    "predictions2 = Dense(2, activation='softmax')(l3) #non-residually connected network\n",
    "\n",
    "\n",
    "model1 = Model(inputs=inputs, outputs=predictions1) #residual connections\n",
    "\n",
    "model2 = Model(inputs=inputs, outputs=predictions2) #non-residually connected network\n",
    "#Compilation\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "                optimizer='Nadam',\n",
    "                metrics=['accuracy', f1])\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "                optimizer='Nadam',\n",
    "                metrics=['accuracy', f1])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model1.fit(x_train, y_train, epochs=40, batch_size=32, validation_split=0.2) #residual connections\n",
    "results1 = model1.predict(x_test, batch_size=32)\n",
    "y_pred1=results1.argmax(axis=-1)\n",
    "\n",
    "model2.fit(x_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "results2 = model2.predict(x_test, batch_size=32)#non-residually connected network\n",
    "y_pred2=results2.argmax(axis=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix for functional model with no residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117,   0],\n",
       "       [  2,  87]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix for functional model with one residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115,   2],\n",
       "       [  0,  89]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
